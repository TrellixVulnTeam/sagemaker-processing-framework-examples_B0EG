{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Processing Script: HuggingFace\n",
    "\n",
    "This notebook shows a very basic example of using SageMaker Processing to create train, test and validation datasets. SageMaker Processing is used to create these datasets, which then are written back to S3.\n",
    "\n",
    "In a nutshell, we will create a `HuggingFaceProcessor` object, passing the HuggingFace Transformer version we want to use, as well as our managed infrastructure requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our use case, we will download a well-known datasets, publicly available online, called the [Amazon Customer Reviews dataset](https://s3.amazonaws.com/amazon-reviews-pds/readme.html). This dataset is composed of 130+ million customer reviews. The data is available in TSV files in the `amazon-reviews-pds` S3 bucket in AWS US East Region. Each line in the data files corresponds to an individual review (tab delimited, with no quote and escape characters). Samples of the data are available in English and French, and we will use both in our demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p .data .output\n",
    "!aws s3 cp s3://amazon-reviews-pds/tsv/sample_us.tsv .data/ \n",
    "!aws s3 cp s3://amazon-reviews-pds/tsv/sample_fr.tsv .data/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import Session\n",
    "\n",
    "session = Session()\n",
    "bucket = session.default_bucket()\n",
    "key_prefix = 'frameworkprocessors/huggingface-example'\n",
    "\n",
    "source_path = session.upload_data('.data', bucket=bucket, key_prefix=f'{key_prefix}/data')\n",
    "source_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the script you'd like to use with Processing with your logic\n",
    "\n",
    "This script is executed by Amazon SageMaker.\n",
    "\n",
    "In the `main`, it does the core of the operations: it reads and parses arguments passed as parameters, unpacks the model file, then loads the model, preprocess, predict, postprocess the data. Remember to write data locally in the final step so that SageMaker can copy them to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize huggingface-processing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Sagemaker Processor \n",
    "\n",
    "Once the data has been uploaded to S3, we can now create the `HuggingFaceProcessor` object. We specify the version of the framework that we want to use, the python version, the role with the correct permissions to read the dataset from S3, and the instances we're planning on using for our processing job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "hfp = HuggingFaceProcessor(\n",
    "    role=get_execution_role(), \n",
    "    instance_count=2,\n",
    "    instance_type='ml.g4dn.xlarge',\n",
    "    transformers_version='4.4.2',\n",
    "    pytorch_version='1.6.0', \n",
    "    base_job_name='frameworkprocessor-hf'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that's left to do is to `run()` the Processing job: we will specify our python script that contains the logic of the transformation in the `code` argument and its dependencies in the `source_dir` folder, the `inputs` and the `outputs` of our job.\n",
    "\n",
    "Note: in the folder indicated in the `source_dir` argument, it is possible to have a `requirements.txt` file with the dependencies of our script. This file will make SageMaker Processing automatically install the packages specified in it by running the `pip install -r requirements.txt` command before launching the job itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfp.run(\n",
    "    code='huggingface-processing.py',\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            input_name='data',\n",
    "            source=source_path,\n",
    "            destination='/opt/ml/processing/input/data/'\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name='train', source='/opt/ml/processing/output/train/'),\n",
    "        ProcessingOutput(output_name='test', source='/opt/ml/processing/output/test/'),\n",
    "        ProcessingOutput(output_name='val', source='/opt/ml/processing/output/val/'),\n",
    "    ],\n",
    "    logs=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now check the results of our processing job, and list the outputs from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = hfp.latest_job.outputs[0].destination\n",
    "\n",
    "!aws s3 ls --recursive $output_path"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9d2b948e37551121c9966918e813855172d09a33f53ab70e27ea27a900ccbb7b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('.env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
