{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Processing Script: MXNet and GluonTS\n",
    "\n",
    "This notebook shows a very basic example of using SageMaker Processing to create train, test and validation datasets. SageMaker Processing is used to create these datasets, which then are written back to S3.\n",
    "\n",
    "In a nutshell, we will create a MXNetProcessor object, passing the MXNet version we want to use, as well as our managed infrastructure requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our use case, we will download a well-known datasets, publicly available online, called the [Numenta Anomaly Benchmark (NAB) dataset](https://github.com/numenta/NAB). This dataset is composed of over 50 labeled real-world and artificial timeseries data files plus a novel scoring mechanism designed for real-time applications. \n",
    "\n",
    "In our example, we will use the volume of tweets for Amazon, and we will process this dataset to make it compatible with MXNet GluonTS library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p .data .output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#getting train datatset of twitter volume\n",
    "url = \"https://raw.githubusercontent.com/numenta/NAB/master/data/realTweets/Twitter_volume_AMZN.csv\"\n",
    "df = pd.read_csv(url, header=0, index_col=0)\n",
    "df.to_csv('.data/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker \n",
    "\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "key_prefix = 'frameworkprocessors/mxnet-example'\n",
    "\n",
    "source_path = session.upload_data('.data', bucket=bucket, key_prefix=f'{key_prefix}/data')\n",
    "source_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the script you'd like to use with Processing with your logic\n",
    "\n",
    "This script is executed by Amazon SageMaker.\n",
    "\n",
    "In the `main`, it does the core of the operations: it reads and parses arguments passed as parameters, unpacks the model file, then loads the model, preprocess, predict, postprocess the data. Remember to write data locally in the final step so that SageMaker can copy them to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize mxnet-gluonts-processing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Sagemaker Processor \n",
    "\n",
    "Once the data has been uploaded to S3, we can now create the `MXNetProcessor` object. We specify the version of the framework that we want to use, the python version, the role with the correct permissions to read the dataset from S3, and the instances we're planning on using for our processing job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.mxnet import MXNetProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "mxp = MXNetProcessor(\n",
    "    framework_version='1.8.0',\n",
    "    py_version='py37',\n",
    "    role=get_execution_role(), \n",
    "    instance_count=1,\n",
    "    instance_type='ml.c5.xlarge',\n",
    "    base_job_name='frameworkprocessor-mxnet'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that's left to do is to `run()` the Processing job: we will specify our python script that contains the logic of the transformation in the `code` argument and its dependencies in the `source_dir` folder, the `inputs` and the `outputs` of our job.\n",
    "\n",
    "Note: in the folder indicated in the `source_dir` argument, it is possible to have a `requirements.txt` file with the dependencies of our script. This file will make SageMaker Processing automatically install the packages specified in it by running the `pip install -r requirements.txt` command before launching the job itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mxp.run(\n",
    "    code='mxnet-gluonts-processing.py',\n",
    "    source_dir='.',\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            input_name='data', source=source_path,\n",
    "            destination='/opt/ml/processing/input/data/'\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name='processed_data', source='/opt/ml/processing/output/')\n",
    "    ],\n",
    "    logs=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now check the results of our processing job, and list the outputs from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = mxp.latest_job.outputs[0].destination\n",
    "\n",
    "!aws s3 ls --recursive $output_path"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "62c64044d4c154360cb75d05b493e8b0d10fa1187381e0ceaf329463ad089ac8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('.env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
